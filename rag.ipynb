{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "OpenAI API Key is not set. Please set it as an environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 220\u001b[0m\n\u001b[1;32m    218\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_key:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI API Key is not set. Please set it as an environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    222\u001b[0m DB_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany_data.db\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m USER_QUERY \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many people have booked at least 50 hours before 01/05/2024?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: OpenAI API Key is not set. Please set it as an environment variable."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import openai\n",
    "\n",
    "# Keyword Extractor\n",
    "class KeywordExtractor:\n",
    "    def __init__(self, method=\"roberta\"):\n",
    "        self.method = method.lower()\n",
    "        if self.method == \"roberta\":\n",
    "            self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "            self.model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        elif self.method == \"ada\":\n",
    "            self.model = \"ada\"  # Ada doesn't use tokenizer/model directly\n",
    "        else:\n",
    "            raise ValueError(\"Supported methods are 'roberta' and 'ada'.\")\n",
    "\n",
    "    def extract_keywords(self, query):\n",
    "        return query.split()  # Simple split (enhance as needed)\n",
    "\n",
    "# Embedding Handler\n",
    "class EmbeddingHandler:\n",
    "    def __init__(self, method=\"roberta\"):\n",
    "        self.method = method.lower()\n",
    "        if self.method == \"roberta\":\n",
    "            self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "            self.model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        elif self.method == \"ada\":\n",
    "            self.model = \"ada\"\n",
    "        else:\n",
    "            raise ValueError(\"Supported methods are 'roberta' and 'ada'.\")\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        if self.method == \"roberta\":\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        elif self.method == \"ada\":\n",
    "            try:\n",
    "                response = openai.Embedding.create(\n",
    "                    input=text,\n",
    "                    model=\"text-embedding-ada-002\"\n",
    "                )\n",
    "                return np.array(response['data'][0]['embedding'])\n",
    "            except openai.error.OpenAIError as e:\n",
    "                print(f\"OpenAI API error: {e}\")\n",
    "                return None\n",
    "\n",
    "    def calculate_similarity(self, emb1, emb2):\n",
    "        return cosine_similarity([emb1], [emb2])[0][0]\n",
    "\n",
    "# Schema Extractor\n",
    "class SchemaExtractor:\n",
    "    def __init__(self, db_file):\n",
    "        self.db_file = db_file\n",
    "\n",
    "    def extract_schema(self):\n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        schema = {}\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = cursor.fetchall()\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "            columns = cursor.fetchall()\n",
    "            schema[table_name] = [col[1] for col in columns]\n",
    "        conn.close()\n",
    "        return schema\n",
    "\n",
    "# Connections\n",
    "class Connections:\n",
    "    def __init__(self, db_file):\n",
    "        self.db_file = db_file\n",
    "\n",
    "    def extract_keys(self, table_name):\n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        primary_keys = []\n",
    "        foreign_keys = []\n",
    "\n",
    "        # Extract primary keys\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "        columns = cursor.fetchall()\n",
    "        for col in columns:\n",
    "            if col[-1] == 1:  # Primary key indicator\n",
    "                primary_keys.append(col[1])\n",
    "\n",
    "        # Extract foreign keys\n",
    "        cursor.execute(f\"PRAGMA foreign_key_list({table_name});\")\n",
    "        keys = cursor.fetchall()\n",
    "        for key in keys:\n",
    "            foreign_keys.append({\n",
    "                \"from_column\": key[3],\n",
    "                \"to_table\": key[2],\n",
    "                \"to_column\": key[4]\n",
    "            })\n",
    "\n",
    "        conn.close()\n",
    "        return primary_keys, foreign_keys\n",
    "\n",
    "# RAG Pipeline\n",
    "class RAGPipeline:\n",
    "    def __init__(self, db_file, user_query, similarity_threshold=0.4, embedding_method=\"roberta\"):\n",
    "        self.db_file = db_file\n",
    "        self.user_query = user_query\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.embedding_handler = EmbeddingHandler(method=embedding_method)\n",
    "        self.keyword_extractor = KeywordExtractor(method=embedding_method)\n",
    "        self.schema_extractor = SchemaExtractor(db_file)\n",
    "        self.schema = self.schema_extractor.extract_schema()\n",
    "        self.connections = Connections(db_file)\n",
    "\n",
    "    def identify_relevant_tables(self, keywords):\n",
    "        results = []\n",
    "        for keyword in keywords:\n",
    "            keyword_embedding = self.embedding_handler.get_embedding(keyword)\n",
    "            tables = []\n",
    "            similarities = []\n",
    "            for table, columns in self.schema.items():\n",
    "                table_embedding = self.embedding_handler.get_embedding(table)\n",
    "                similarity = self.embedding_handler.calculate_similarity(\n",
    "                    table_embedding, keyword_embedding\n",
    "                )\n",
    "                if similarity >= self.similarity_threshold:\n",
    "                    tables.append(table)\n",
    "                    similarities.append(similarity)\n",
    "            results.append({\n",
    "                \"Keyword\": keyword,\n",
    "                \"Identified Tables\": tables,\n",
    "                \"Similarity Scores\": similarities\n",
    "            })\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def identify_relevant_columns(self, keywords):\n",
    "        results = []\n",
    "        for keyword in keywords:\n",
    "            keyword_embedding = self.embedding_handler.get_embedding(keyword)\n",
    "            columns = []\n",
    "            similarities = []\n",
    "            for table, cols in self.schema.items():\n",
    "                for column in cols:\n",
    "                    column_name = f\"{table}.{column}\"\n",
    "                    column_embedding = self.embedding_handler.get_embedding(column_name)\n",
    "                    similarity = self.embedding_handler.calculate_similarity(\n",
    "                        column_embedding, keyword_embedding\n",
    "                    )\n",
    "                    if similarity >= self.similarity_threshold:\n",
    "                        columns.append(column_name)\n",
    "                        similarities.append(similarity)\n",
    "            results.append({\n",
    "                \"Keyword\": keyword,\n",
    "                \"Identified Columns\": columns,\n",
    "                \"Similarity Scores\": similarities\n",
    "            })\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def build_subgraph(self, table_similarity_df, column_similarity_df):\n",
    "        graph = nx.Graph()\n",
    "\n",
    "        # Add nodes for tables\n",
    "        for index, row in table_similarity_df.iterrows():\n",
    "            for table in row[\"Identified Tables\"]:\n",
    "                graph.add_node(table, type=\"table\")\n",
    "\n",
    "        # Add nodes for columns and edges to their tables\n",
    "        for index, row in column_similarity_df.iterrows():\n",
    "            for column in row[\"Identified Columns\"]:\n",
    "                table_name = column.split('.')[0]\n",
    "                graph.add_node(column, type=\"column\")\n",
    "                graph.add_edge(table_name, column)\n",
    "\n",
    "        # Add edges between tables based on foreign keys\n",
    "        for table in self.schema.keys():\n",
    "            _, foreign_keys = self.connections.extract_keys(table)\n",
    "            for fk in foreign_keys:\n",
    "                graph.add_edge(table, fk[\"to_table\"],\n",
    "                               from_column=fk[\"from_column\"],\n",
    "                               to_column=fk[\"to_column\"])\n",
    "\n",
    "        print(\"Subgraph Nodes:\")\n",
    "        print(graph.nodes(data=True))\n",
    "\n",
    "        print(\"Subgraph Edges:\")\n",
    "        print(graph.edges(data=True))\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Extracting keywords...\")\n",
    "        keywords = self.keyword_extractor.extract_keywords(self.user_query)\n",
    "        print(f\"Keywords: {keywords}\")\n",
    "\n",
    "        print(\"Identifying relevant tables...\")\n",
    "        table_similarity_df = self.identify_relevant_tables(keywords)\n",
    "        print(table_similarity_df)\n",
    "\n",
    "        print(\"Identifying relevant columns...\")\n",
    "        column_similarity_df = self.identify_relevant_columns(keywords)\n",
    "        print(column_similarity_df)\n",
    "\n",
    "        print(\"Building subgraph...\")\n",
    "        subgraph = self.build_subgraph(table_similarity_df, column_similarity_df)\n",
    "\n",
    "        return table_similarity_df, column_similarity_df, subgraph\n",
    "variable = 1\n",
    "# Main Runner\n",
    "if __name__ == \"__main__\":\n",
    "    # Set OpenAI API Key\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not openai.api_key:\n",
    "        raise ValueError(\"OpenAI API Key is not set. Please set it as an environment variable.\")\n",
    "\n",
    "    DB_FILE = \"company_data.db\"\n",
    "    USER_QUERY = \"How many people have booked at least 50 hours before 01/05/2024?\"\n",
    "    EMBEDDING_METHOD = \"ada\"  # Change to \"roberta\" or \"ada\"\n",
    "\n",
    "    pipeline = RAGPipeline(DB_FILE, USER_QUERY, embedding_method=EMBEDDING_METHOD)\n",
    "    table_similarity, column_similarity, subgraph = pipeline.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
