{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting keywords...\n",
      "Keywords: ['how many people', 'Grade F managers']\n",
      "Identifying relevant tables and calculating similarity scores...\n",
      "Similarity Scores and Keys as Table:\n",
      "         Keyword Identified Tables Similarity Scores\n",
      " how many people          [people]      [0.59598446]\n",
      "Grade F managers                []                []\n",
      "Identifying relevant columns and calculating similarity scores...\n",
      "Similarity Scores for Columns as Table:\n",
      "         Keyword                              Identified Columns                   Similarity Scores\n",
      " how many people [people.full_name, people.fte, people.division] [0.40601167, 0.4373518, 0.42937395]\n",
      "Grade F managers                     [people.line_manager_grade]                        [0.52981186]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from rank_bm25 import BM25Okapi\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Keyword Extractor\n",
    "class KeywordExtractor:\n",
    "    def __init__(self, method=\"sentence_transformer\"):\n",
    "        self.method = method\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")  # Load spaCy for phrase extraction\n",
    "        if method == \"sentence_transformer\":\n",
    "            self.tokenizer = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        else:\n",
    "            raise ValueError(\"Supported method: 'sentence_transformer'.\")\n",
    "\n",
    "    def extract_keywords(self, query):\n",
    "        doc = self.nlp(query)\n",
    "        phrases = [chunk.text for chunk in doc.noun_chunks]  # Extract noun phrases\n",
    "        return phrases\n",
    "\n",
    "# Embedding Handler\n",
    "class EmbeddingHandler:\n",
    "    def __init__(self, method=\"sentence_transformer\"):\n",
    "        self.method = method\n",
    "        if method == \"sentence_transformer\":\n",
    "            self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        else:\n",
    "            raise ValueError(\"Supported method: 'sentence_transformer'.\")\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        return self.model.encode(text)\n",
    "\n",
    "    def calculate_similarity(self, emb1, emb2, metric=\"cosine\", **kwargs):\n",
    "        if metric == \"cosine\":\n",
    "            return cosine_similarity([emb1], [emb2])[0][0]\n",
    "        elif metric == \"euclidean\":\n",
    "            return -np.linalg.norm(np.array(emb1) - np.array(emb2))\n",
    "        elif metric == \"manhattan\":\n",
    "            return -np.sum(np.abs(np.array(emb1) - np.array(emb2)))\n",
    "        elif metric == \"jaccard\":\n",
    "            set1, set2 = set(kwargs.get(\"set1\", [])), set(kwargs.get(\"set2\", []))\n",
    "            if not set1 or not set2:  # Handle empty sets\n",
    "                return 0  # Return 0 similarity for empty comparisons\n",
    "            return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "        elif metric == \"levenshtein\":\n",
    "            str1, str2 = kwargs.get(\"str1\", \"\"), kwargs.get(\"str2\", \"\")\n",
    "            if not str1 or not str2:  # Handle empty strings\n",
    "                return 0  # Return 0 similarity for empty comparisons\n",
    "            max_len = max(len(str1), len(str2))\n",
    "            return 1 - levenshtein_distance(str1, str2) / max_len\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported similarity metric.\")\n",
    "\n",
    "# Schema Extractor\n",
    "class SchemaExtractor:\n",
    "    def __init__(self, db_file):\n",
    "        self.db_file = db_file\n",
    "\n",
    "    def extract_schema(self):\n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        schema = {}\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = cursor.fetchall()\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "            columns = cursor.fetchall()\n",
    "            schema[table_name] = [col[1] for col in columns]\n",
    "        conn.close()\n",
    "        return schema\n",
    "\n",
    "# Connections\n",
    "class Connections:\n",
    "    def __init__(self, db_file):\n",
    "        self.db_file = db_file\n",
    "\n",
    "    def extract_keys(self, table_name):\n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        primary_keys = []\n",
    "        foreign_keys = []\n",
    "\n",
    "        # Extract primary keys\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "        columns = cursor.fetchall()\n",
    "        for col in columns:\n",
    "            if col[-1] == 1:  # Primary key indicator\n",
    "                primary_keys.append(col[1])\n",
    "\n",
    "        # Extract foreign keys\n",
    "        cursor.execute(f\"PRAGMA foreign_key_list({table_name});\")\n",
    "        keys = cursor.fetchall()\n",
    "        for key in keys:\n",
    "            foreign_keys.append({\n",
    "                \"from_column\": key[3],\n",
    "                \"to_table\": key[2],\n",
    "                \"to_column\": key[4]\n",
    "            })\n",
    "\n",
    "        conn.close()\n",
    "        return primary_keys, foreign_keys\n",
    "\n",
    "# RAG Pipeline\n",
    "class RAGPipeline:\n",
    "    def __init__(self, db_file, user_query, method=\"sentence_transformer\", similarity_metric=\"cosine\", similarity_threshold=0.4):\n",
    "        self.db_file = db_file\n",
    "        self.user_query = user_query\n",
    "        self.similarity_metric = similarity_metric\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.embedding_handler = EmbeddingHandler(method=method)\n",
    "        self.keyword_extractor = KeywordExtractor(method=method)\n",
    "        self.schema_extractor = SchemaExtractor(db_file)\n",
    "        self.schema = self.schema_extractor.extract_schema()\n",
    "        self.connections = Connections(db_file)\n",
    "\n",
    "    def identify_relevant_tables(self, keywords):\n",
    "        results = []\n",
    "        for keyword in keywords:\n",
    "            keyword_embedding = self.embedding_handler.get_embedding(keyword)\n",
    "            tables = []\n",
    "            similarities = []\n",
    "            for table, columns in self.schema.items():\n",
    "                table_embedding = self.embedding_handler.get_embedding(table)\n",
    "                similarity = self.embedding_handler.calculate_similarity(\n",
    "                    table_embedding, keyword_embedding, metric=self.similarity_metric,\n",
    "                    str1=table, str2=keyword  # Pass strings for Levenshtein\n",
    "                )\n",
    "                if similarity >= self.similarity_threshold:\n",
    "                    tables.append(table)\n",
    "                    similarities.append(similarity)\n",
    "            results.append({\n",
    "                \"Keyword\": keyword,\n",
    "                \"Identified Tables\": tables,\n",
    "                \"Similarity Scores\": similarities\n",
    "            })\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def identify_relevant_columns(self, keywords):\n",
    "        \"\"\"\n",
    "        Identify relevant columns in the database schema based on the extracted keywords,\n",
    "        independently of the relevance of the parent table.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for keyword in keywords:\n",
    "            keyword_embedding = self.embedding_handler.get_embedding(keyword)\n",
    "            columns = []\n",
    "            similarities = []\n",
    "            for table, cols in self.schema.items():  # Iterate through all tables and columns\n",
    "                for column in cols:\n",
    "                    column_name = f\"{table}.{column}\"\n",
    "                    column_embedding = self.embedding_handler.get_embedding(column_name)\n",
    "                    similarity = self.embedding_handler.calculate_similarity(\n",
    "                        column_embedding, keyword_embedding, metric=self.similarity_metric,\n",
    "                        str1=column_name, str2=keyword  # Pass strings for Levenshtein\n",
    "                    )\n",
    "                    if similarity >= self.similarity_threshold:\n",
    "                        columns.append(column_name)\n",
    "                        similarities.append(similarity)\n",
    "            # Sort columns by similarity scores (descending)\n",
    "            sorted_columns = sorted(zip(columns, similarities), key=lambda x: x[1], reverse=True)\n",
    "            results.append({\n",
    "                \"Keyword\": keyword,\n",
    "                \"Identified Columns\": [col[0] for col in sorted_columns],\n",
    "                \"Similarity Scores\": [col[1] for col in sorted_columns]\n",
    "            })\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Extracting keywords...\")\n",
    "        keywords = self.keyword_extractor.extract_keywords(self.user_query)\n",
    "        print(f\"Keywords: {keywords}\")\n",
    "\n",
    "        print(\"Identifying relevant tables and calculating similarity scores...\")\n",
    "        table_similarity_df = self.identify_relevant_tables(keywords)\n",
    "\n",
    "        print(\"Similarity Scores and Keys as Table:\")\n",
    "        print(table_similarity_df.to_string(index=False))\n",
    "\n",
    "        print(\"Identifying relevant columns and calculating similarity scores...\")\n",
    "        column_similarity_df = self.identify_relevant_columns(keywords)\n",
    "\n",
    "        print(\"Similarity Scores for Columns as Table:\")\n",
    "        print(column_similarity_df.to_string(index=False))\n",
    "\n",
    "        return table_similarity_df, column_similarity_df\n",
    "\n",
    "# Main Runner\n",
    "if __name__ == \"__main__\":\n",
    "    DB_FILE = \"company_data.db\"\n",
    "    USER_QUERY = \"on average how many people are managed by Grade F managers?\"\n",
    "    METHOD = \"sentence_transformer\"  # Use sentence-level embedding\n",
    "    SIMILARITY_METRIC = \"cosine\"  # Choose from \"cosine\", \"euclidean\", \"manhattan\", \"jaccard\", \"levenshtein\"\n",
    "\n",
    "    pipeline = RAGPipeline(DB_FILE, USER_QUERY, method=METHOD, similarity_metric=SIMILARITY_METRIC)\n",
    "    table_similarity_scores, column_similarity_scores = pipeline.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Schema:\n",
      "{'art': ['resource_id',\n",
      "         'full_name',\n",
      "         'group',\n",
      "         'start_date',\n",
      "         'compliance_program',\n",
      "         'course_name',\n",
      "         'course_completion_date',\n",
      "         'art_due_date',\n",
      "         'course_completion_status',\n",
      "         'final_art_status',\n",
      "         'wd_file_date',\n",
      "         'run_date'],\n",
      " 'holiday_balance': ['colleague_id',\n",
      "                     'report_date',\n",
      "                     'carried_forward',\n",
      "                     'accrued',\n",
      "                     'entitlement',\n",
      "                     'booked',\n",
      "                     'taken',\n",
      "                     'booked_not_yet_taken',\n",
      "                     'unbooked',\n",
      "                     'untaken',\n",
      "                     'using_workday',\n",
      "                     'hours_not_using_workday',\n",
      "                     'Unbooked_ex_nuw'],\n",
      " 'holiday_details': ['colleague_id',\n",
      "                     'hours',\n",
      "                     'date',\n",
      "                     'entered_on',\n",
      "                     'report_date',\n",
      "                     'taken_tf'],\n",
      " 'people': ['resource_id',\n",
      "            'full_name',\n",
      "            'email',\n",
      "            'first_name',\n",
      "            'last_name',\n",
      "            'direct_line_manager_id',\n",
      "            'direct_line_manager_name',\n",
      "            'line_manager_email',\n",
      "            'location_name',\n",
      "            'data_source',\n",
      "            'hub_location',\n",
      "            'is_onshore',\n",
      "            'fte',\n",
      "            'scheduled_working_hours',\n",
      "            'grade',\n",
      "            'job_title',\n",
      "            'job_code',\n",
      "            'business_title',\n",
      "            'cost_centre_id',\n",
      "            'agency',\n",
      "            'original_tenure_start',\n",
      "            'current_start_date',\n",
      "            'leave_date',\n",
      "            'contractor_day_rate_banding',\n",
      "            'on_leave',\n",
      "            'job_family',\n",
      "            'worker_type',\n",
      "            'high_lev_workertype',\n",
      "            'funding_source',\n",
      "            'division',\n",
      "            'cc_layer_1',\n",
      "            'cc_layer_2',\n",
      "            'cc_layer_3',\n",
      "            'cc_layer_4',\n",
      "            'is_manager',\n",
      "            'post_code',\n",
      "            'city',\n",
      "            'location_id',\n",
      "            'position_id',\n",
      "            'workday_job_family_group',\n",
      "            'workday_job_category',\n",
      "            'compensation_grade_profile',\n",
      "            'oim_learner',\n",
      "            'workday_learner',\n",
      "            'learner_type',\n",
      "            'layer_2',\n",
      "            'layer_3',\n",
      "            'layer_4',\n",
      "            'layer_5',\n",
      "            'layer_6',\n",
      "            'layer_7',\n",
      "            'layer_8',\n",
      "            'layer_9',\n",
      "            'layer_10',\n",
      "            'layer_11',\n",
      "            'layer_12',\n",
      "            'layer_13',\n",
      "            'layer_2_file_id',\n",
      "            'layer_3_file_id',\n",
      "            'layer_4_file_id',\n",
      "            'layer_5_file_id',\n",
      "            'layer_6_file_id',\n",
      "            'layer_7_file_id',\n",
      "            'layer_8_file_id',\n",
      "            'layer_9_file_id',\n",
      "            'layer_10_file_id',\n",
      "            'layer_11_file_id',\n",
      "            'layer_12_file_id',\n",
      "            'layer_13_file_id',\n",
      "            'colleague_layer',\n",
      "            'line_manager_grade',\n",
      "            'line_manager_business_title']}\n"
     ]
    }
   ],
   "source": [
    "# Print the entire schema\n",
    "from pprint import pprint\n",
    "\n",
    "def print_schema(db_file):\n",
    "    schema_extractor = SchemaExtractor(db_file)\n",
    "    schema = schema_extractor.extract_schema()\n",
    "    pprint(schema)\n",
    "\n",
    "# Example usage\n",
    "DB_FILE = \"company_data.db\"\n",
    "print(\"Database Schema:\")\n",
    "print_schema(DB_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key is registered.\n"
     ]
    }
   ],
   "source": [
    "# Check if the OpenAI API key is registered in the environment\n",
    "import os\n",
    "\n",
    "# Attempt to retrieve the OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"OpenAI API key is registered.\")\n",
    "else:\n",
    "    print(\"OpenAI API key is NOT registered. Please set it correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people,\"people.direct_line_manager_id, people.resource_id, people.grade\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
