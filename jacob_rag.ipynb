{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('people.resource_id', \"Colleague's GID, the unique number assigned to each colleague. **No NULL values observed**.\")\n",
      "('people.full_name', \"Colleague's first and last names\")\n",
      "('people.email', \"Colleague's LBG email address\")\n",
      "('people.first_name', \"Colleague's first name\")\n",
      "('people.last_name', \"Colleague's last name\")\n",
      "('people.direct_line_manager_id', \"GID the unique number assigned assigned to colleague's line manager\")\n",
      "('people.direct_line_manager_name', \"first and last names of colleague's line manager\")\n",
      "('people.line_manager_email', \"LBG email address of colleague's line manager\")\n",
      "('people.location_name', 'LBG building colleague primarily works out of')\n",
      "('people.data_source', \"System where colleague's data is mastered; Possible values: ('Workday', 'Beeline', 'OIM-Beeline','Workday_Advanced', 'Workday Headcount Prompt'). **NULL values observed - requires further investigation to determine the source.**\")\n",
      "('people.hub_location', \"Identifies the regional hub that a colleague's primary work location is part of\")\n",
      "('people.is_onshore', 'Identifies if a colleague is based in the UK (onshore) or elsewhere (offshore)')\n",
      "('people.fte', 'Identifies whether a colleague is Full Time (FTE = 1) or on reduced hours (FTE < 1')\n",
      "('art.resource_id', \"Colleague's GID, the unique number assigned to each colleague. WARNING: This field contains duplicates with the top 5 most frequent values being:  99067269, 98414660, 97710479, 97563946, and 97343750. Avoid using as a primary key.\")\n",
      "('art.full_name', \"Colleague's first and last names\")\n",
      "('art.group', 'The area of the group the colleague is part of')\n",
      "('art.start_date', 'The date the employee started working for the group')\n",
      "('art.compliance_program', \"Name of the training compliance program. Should be limited to 'Q3 2024 - ART' (112648 records) and 'Q2 2024 - ART' (172814 records). WARNING: This column contains inconsistent values.\")\n",
      "('art.course_name', 'The individual course name in the program')\n",
      "('art.course_completion_date', 'The date the course was completed. WARNING: There are data quality issues where the course completion date is later than the art due date.')\n",
      "('art.art_due_date', 'The date the compliance program is due by')\n",
      "('art.course_completion_status', \"The registration or completion status for the course. It has possible values of  'not registered' (71375 records), 'completed' (204886 records), and 'optional' (9201 records). WARNING: This column contains inconsistent values.\")\n",
      "('art.final_art_status', \"Indicator if the program has been completed. Possible values include:  'Completed' (199970 records), 'Not Started' (63894 records), 'In Progres' (6025 records), 'Not started' (63894 records), 'Known Exception' (5654 records), 'Overdue' (5253 records), and 'Completed late' (4666 records).\")\n",
      "('art.wd_file_date', 'Workday file date of report extraction')\n",
      "('art.run_date', 'The date on which the report was generated')\n",
      "('holiday_balance.colleague_id', \"Colleague's unique identifier. Each ID represents a snapshot of their holiday balance on the report date. Note: Multiple entries for the same colleague ID on a single report date indicate potential data duplication and require further investigation.\")\n",
      "('holiday_balance.report_date', \"The date when the holiday balance snapshot was taken. This field is crucial for time-series analysis. Note: Currently, there is only one distinct report date ('2024-07-22') in the dataset, indicating a potential limitation for historical analysis.\")\n",
      "('holiday_balance.carried_forward', \"The number of holiday hours carried over from the previous year, contributing to the colleague's total holiday entitlement for the current year.\")\n",
      "('holiday_balance.accrued', 'The number of holiday hours accrued by the colleague in the current year, up to the report date.')\n",
      "('holiday_balance.entitlement', 'The total holiday entitlement for the year, encompassing any additional days purchased or carried forward. This represents the maximum number of holiday hours a colleague is entitled to take for the year. Note: No colleague has taken more holiday than their entitlement.')\n",
      "('holiday_balance.booked', 'The total number of holiday hours booked, including both hours that have been taken and those yet to be taken.')\n",
      "('holiday_balance.taken', 'The number of holiday hours already taken by the colleague.')\n",
      "('holiday_balance.booked_not_yet_taken', 'The number of holiday hours booked but not yet taken. This represents the holiday hours committed to but not yet utilized.')\n",
      "('holiday_balance.unbooked', 'The remaining holiday hours available for the colleague to book, calculated as \"entitlement\" - \"booked\".')\n",
      "('holiday_balance.untaken', 'The total holiday hours the colleague still has available, including \"booked_not_yet_taken\" and \"unbooked\" hours.')\n",
      "('holiday_balance.using_workday', 'A flag indicating whether the colleague has booked any holiday in the Workday system. A value of 1 signifies that at least one holiday booking was made through Workday. It\\'s essential to note that this field doesn\\'t reflect the total number of hours booked through Workday, only the presence or absence of such bookings. For a comprehensive understanding of holiday booked outside Workday, refer to \"hours_not_using_workday\".')\n",
      "('holiday_balance.hours_not_using_workday', 'The number of holiday hours not booked through the Workday system. This field warrants further analysis to determine the reasons behind booking holidays outside the designated system and identify any potential limitations or data discrepancies. Further investigation is needed to understand why some colleagues book holidays outside of the Workday system and whether this data is consistently captured.')\n",
      "('holiday_balance.Unbooked_ex_nuw', 'Represents the remaining holiday hours available for booking, excluding any hours not booked in the Workday system (\"hours_not_using_workday\"). This field offers a more conservative estimate of available holiday hours by considering only those booked within the Workday system, providing a more accurate reflection of manageable holiday balances.')\n",
      "('holiday_details.colleague_id', \"Colleague's unique identifier. This is a foreign key that should link to the 'colleague' table. This ID is consistent across related tables and is crucial for linking holiday data to specific colleagues. No null values are allowed in this column.\")\n",
      "('holiday_details.hours', 'Total holiday hours booked by the colleague. Values can be positive or negative. Negative values indicate a cancellation of the holiday.')\n",
      "('holiday_details.date', 'The date for which holiday hours were booked (YYYY-MM-DD). The data in this column ranges from 2024-01-01 to 2024-12-31.')\n",
      "('holiday_details.entered_on', \"The date when the holiday was recorded in the system (YYYY-MM-DD). It's crucial to ensure this date is not later than the holiday 'date'. Data entry errors have been observed where this date is later than the holiday date.\")\n",
      "('holiday_details.report_date', 'snapshot date of the data (YYYY-MM-DD). Currently, the table contains data only for 2024-07-22')\n",
      "('holiday_details.taken_tf', \"A binary flag (0 or 1) indicating if the holiday hours were actually taken. This field often differs from 'hours', suggesting a significant discrepancy between booked and actual holiday taken (221087 instances). It's unclear if 'hours' represents the intended duration while 'taken_tf' reflects the actual time off.\")\n"
     ]
    }
   ],
   "source": [
    "schema_descriptions = {'people': {'resource_id': \"Colleague's GID, the unique number assigned to each colleague. **No NULL values observed**.\", 'full_name': \"Colleague's first and last names\", 'email': \"Colleague's LBG email address\", 'first_name': \"Colleague's first name\", 'last_name': \"Colleague's last name\", 'direct_line_manager_id': \"GID the unique number assigned assigned to colleague's line manager\", 'direct_line_manager_name': \"first and last names of colleague's line manager\", 'line_manager_email': \"LBG email address of colleague's line manager\", 'location_name': 'LBG building colleague primarily works out of', 'data_source': \"System where colleague's data is mastered; Possible values: ('Workday', 'Beeline', 'OIM-Beeline','Workday_Advanced', 'Workday Headcount Prompt'). **NULL values observed - requires further investigation to determine the source.**\", 'hub_location': \"Identifies the regional hub that a colleague's primary work location is part of\", 'is_onshore': 'Identifies if a colleague is based in the UK (onshore) or elsewhere (offshore)', 'fte': 'Identifies whether a colleague is Full Time (FTE = 1) or on reduced hours (FTE < 1'}, 'art': {'resource_id': \"Colleague's GID, the unique number assigned to each colleague. WARNING: This field contains duplicates with the top 5 most frequent values being:  99067269, 98414660, 97710479, 97563946, and 97343750. Avoid using as a primary key.\", 'full_name': \"Colleague's first and last names\", 'group': 'The area of the group the colleague is part of', 'start_date': 'The date the employee started working for the group', 'compliance_program': \"Name of the training compliance program. Should be limited to 'Q3 2024 - ART' (112648 records) and 'Q2 2024 - ART' (172814 records). WARNING: This column contains inconsistent values.\", 'course_name': 'The individual course name in the program', 'course_completion_date': 'The date the course was completed. WARNING: There are data quality issues where the course completion date is later than the art due date.', 'art_due_date': 'The date the compliance program is due by', 'course_completion_status': \"The registration or completion status for the course. It has possible values of  'not registered' (71375 records), 'completed' (204886 records), and 'optional' (9201 records). WARNING: This column contains inconsistent values.\", 'final_art_status': \"Indicator if the program has been completed. Possible values include:  'Completed' (199970 records), 'Not Started' (63894 records), 'In Progres' (6025 records), 'Not started' (63894 records), 'Known Exception' (5654 records), 'Overdue' (5253 records), and 'Completed late' (4666 records).\", 'wd_file_date': 'Workday file date of report extraction', 'run_date': 'The date on which the report was generated'}, 'holiday_balance': {'colleague_id': \"Colleague's unique identifier. Each ID represents a snapshot of their holiday balance on the report date. Note: Multiple entries for the same colleague ID on a single report date indicate potential data duplication and require further investigation.\", 'report_date': \"The date when the holiday balance snapshot was taken. This field is crucial for time-series analysis. Note: Currently, there is only one distinct report date ('2024-07-22') in the dataset, indicating a potential limitation for historical analysis.\", 'carried_forward': \"The number of holiday hours carried over from the previous year, contributing to the colleague's total holiday entitlement for the current year.\", 'accrued': 'The number of holiday hours accrued by the colleague in the current year, up to the report date.', 'entitlement': 'The total holiday entitlement for the year, encompassing any additional days purchased or carried forward. This represents the maximum number of holiday hours a colleague is entitled to take for the year. Note: No colleague has taken more holiday than their entitlement.', 'booked': 'The total number of holiday hours booked, including both hours that have been taken and those yet to be taken.', 'taken': 'The number of holiday hours already taken by the colleague.', 'booked_not_yet_taken': 'The number of holiday hours booked but not yet taken. This represents the holiday hours committed to but not yet utilized.', 'unbooked': 'The remaining holiday hours available for the colleague to book, calculated as \"entitlement\" - \"booked\".', 'untaken': 'The total holiday hours the colleague still has available, including \"booked_not_yet_taken\" and \"unbooked\" hours.', 'using_workday': 'A flag indicating whether the colleague has booked any holiday in the Workday system. A value of 1 signifies that at least one holiday booking was made through Workday. It\\'s essential to note that this field doesn\\'t reflect the total number of hours booked through Workday, only the presence or absence of such bookings. For a comprehensive understanding of holiday booked outside Workday, refer to \"hours_not_using_workday\".', 'hours_not_using_workday': 'The number of holiday hours not booked through the Workday system. This field warrants further analysis to determine the reasons behind booking holidays outside the designated system and identify any potential limitations or data discrepancies. Further investigation is needed to understand why some colleagues book holidays outside of the Workday system and whether this data is consistently captured.', 'Unbooked_ex_nuw': 'Represents the remaining holiday hours available for booking, excluding any hours not booked in the Workday system (\"hours_not_using_workday\"). This field offers a more conservative estimate of available holiday hours by considering only those booked within the Workday system, providing a more accurate reflection of manageable holiday balances.'}, 'holiday_details': {'colleague_id': \"Colleague's unique identifier. This is a foreign key that should link to the 'colleague' table. This ID is consistent across related tables and is crucial for linking holiday data to specific colleagues. No null values are allowed in this column.\", 'hours': 'Total holiday hours booked by the colleague. Values can be positive or negative. Negative values indicate a cancellation of the holiday.', 'date': 'The date for which holiday hours were booked (YYYY-MM-DD). The data in this column ranges from 2024-01-01 to 2024-12-31.', 'entered_on': \"The date when the holiday was recorded in the system (YYYY-MM-DD). It's crucial to ensure this date is not later than the holiday 'date'. Data entry errors have been observed where this date is later than the holiday date.\", 'report_date': 'snapshot date of the data (YYYY-MM-DD). Currently, the table contains data only for 2024-07-22', 'taken_tf': \"A binary flag (0 or 1) indicating if the holiday hours were actually taken. This field often differs from 'hours', suggesting a significant discrepancy between booked and actual holiday taken (221087 instances). It's unclear if 'hours' represents the intended duration while 'taken_tf' reflects the actual time off.\"}}\n",
    "for table, cols in schema_descriptions.items():\n",
    "    for column, description in cols.items():\n",
    "        column_name = f\"{table}.{column}\", description\n",
    "        print(column_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running pipeline with model: roberta-base\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying relevant tables and calculating similarity scores...\n",
      "Similarity Scores and Keys as Table:\n",
      "          Table      Similarity\n",
      "       0.930183          people\n",
      "        0.92003             art\n",
      "       0.950372 holiday_balance\n",
      "holiday_details        0.948806\n",
      "Identifying relevant columns and calculating similarity scores...\n",
      "Similarity Scores for Columns as Table:\n",
      "                                 Column  Similarity\n",
      "        holiday_balance.Unbooked_ex_nuw    0.969383\n",
      "         people.scheduled_working_hours    0.968801\n",
      "   holiday_balance.booked_not_yet_taken    0.968609\n",
      "holiday_balance.hours_not_using_workday    0.968503\n",
      "        holiday_balance.carried_forward    0.968083\n",
      "                  people.cost_centre_id    0.967494\n",
      "     people.contractor_day_rate_banding    0.966308\n",
      "                       people.last_name    0.966044\n",
      "      people.compensation_grade_profile    0.965854\n",
      "            holiday_balance.report_date    0.965588\n",
      "              people.current_start_date    0.965362\n",
      "               holiday_balance.unbooked    0.964936\n",
      "                      people.first_name    0.964750\n",
      "             holiday_details.entered_on    0.964636\n",
      "            people.workday_job_category    0.964509\n",
      "            holiday_details.report_date    0.964472\n",
      "           holiday_balance.colleague_id    0.964293\n",
      "          holiday_balance.using_workday    0.963968\n",
      "           holiday_details.colleague_id    0.963900\n",
      "                        people.on_leave    0.963651\n",
      "            holiday_balance.entitlement    0.963632\n",
      "        people.workday_job_family_group    0.963608\n",
      "                       people.full_name    0.963037\n",
      "                      people.is_onshore    0.963018\n",
      "                 people.workday_learner    0.963011\n",
      "           people.original_tenure_start    0.962986\n",
      "     people.line_manager_business_title    0.962851\n",
      "                   people.location_name    0.962768\n",
      "                 holiday_balance.booked    0.962737\n",
      "               holiday_details.taken_tf    0.962483\n",
      "                      people.leave_date    0.962217\n",
      "                   holiday_details.date    0.962000\n",
      "             art.course_completion_date    0.961781\n",
      "                       people.job_title    0.961640\n",
      "              people.line_manager_email    0.961558\n",
      "        people.direct_line_manager_name    0.961459\n",
      "                  holiday_details.hours    0.961371\n",
      "                holiday_balance.accrued    0.961178\n",
      "                    people.learner_type    0.961144\n",
      "                people.layer_12_file_id    0.960758\n",
      "                people.layer_10_file_id    0.960585\n",
      "                  holiday_balance.taken    0.960503\n",
      "                 people.colleague_layer    0.960496\n",
      "                 people.layer_9_file_id    0.960493\n",
      "                 people.layer_7_file_id    0.960479\n",
      "                 people.layer_2_file_id    0.960296\n",
      "                people.layer_11_file_id    0.960273\n",
      "                     people.location_id    0.960267\n",
      "                       art.art_due_date    0.960239\n",
      "          people.direct_line_manager_id    0.960207\n",
      "                people.layer_13_file_id    0.960095\n",
      "                 people.layer_8_file_id    0.959998\n",
      "                     people.worker_type    0.959951\n",
      "                  people.business_title    0.959814\n",
      "                     people.resource_id    0.959777\n",
      "              people.line_manager_grade    0.959716\n",
      "                 people.layer_5_file_id    0.959653\n",
      "                 people.layer_4_file_id    0.959515\n",
      "                 people.layer_6_file_id    0.959458\n",
      "                      people.is_manager    0.959300\n",
      "                        people.job_code    0.959201\n",
      "                     people.oim_learner    0.959196\n",
      "           art.course_completion_status    0.959182\n",
      "                     people.position_id    0.958824\n",
      "                 people.layer_3_file_id    0.958728\n",
      "                      people.job_family    0.958488\n",
      "             people.high_lev_workertype    0.958042\n",
      "                    people.hub_location    0.957983\n",
      "                  people.funding_source    0.957960\n",
      "                   art.final_art_status    0.957923\n",
      "                     people.data_source    0.957722\n",
      "                      people.cc_layer_1    0.957719\n",
      "                holiday_balance.untaken    0.957552\n",
      "                      people.cc_layer_2    0.957239\n",
      "                         art.start_date    0.956249\n",
      "                       people.post_code    0.956163\n",
      "                      people.cc_layer_4    0.955585\n",
      "                           art.run_date    0.955119\n",
      "                       art.wd_file_date    0.955097\n",
      "                         people.layer_2    0.955056\n",
      "                      people.cc_layer_3    0.954980\n",
      "                          art.full_name    0.954781\n",
      "                        art.resource_id    0.954311\n",
      "                        people.layer_12    0.953493\n",
      "                         people.layer_8    0.953127\n",
      "                        people.layer_13    0.952850\n",
      "                        people.layer_10    0.952533\n",
      "                        people.layer_11    0.952285\n",
      "                         people.layer_4    0.951878\n",
      "                         people.layer_9    0.951423\n",
      "                         people.layer_3    0.951398\n",
      "                         people.layer_6    0.951192\n",
      "                         people.layer_5    0.950888\n",
      "                 art.compliance_program    0.950593\n",
      "                         people.layer_7    0.950494\n",
      "                        art.course_name    0.950042\n",
      "                             people.fte    0.948218\n",
      "                           people.email    0.947287\n",
      "                            people.city    0.946758\n",
      "                          people.agency    0.944575\n",
      "                        people.division    0.943190\n",
      "                           people.grade    0.940811\n",
      "                              art.group    0.940692\n",
      "\n",
      "Running pipeline with model: all-mpnet-base-v2\n",
      "\n",
      "Identifying relevant tables and calculating similarity scores...\n",
      "Similarity Scores and Keys as Table:\n",
      "          Table      Similarity\n",
      "       0.213967          people\n",
      "       0.121346             art\n",
      "       0.399986 holiday_balance\n",
      "holiday_details        0.353063\n",
      "Identifying relevant columns and calculating similarity scores...\n",
      "Similarity Scores for Columns as Table:\n",
      "                                 Column  Similarity\n",
      "   holiday_balance.booked_not_yet_taken    0.414942\n",
      "                 holiday_balance.booked    0.411948\n",
      "               holiday_balance.unbooked    0.409985\n",
      "        holiday_balance.Unbooked_ex_nuw    0.395579\n",
      "                  holiday_details.hours    0.385625\n",
      "     people.contractor_day_rate_banding    0.374817\n",
      "            holiday_balance.report_date    0.367199\n",
      "         people.scheduled_working_hours    0.356954\n",
      "                      people.leave_date    0.351601\n",
      "              people.current_start_date    0.344351\n",
      "                holiday_balance.untaken    0.338170\n",
      "holiday_balance.hours_not_using_workday    0.333376\n",
      "            holiday_details.report_date    0.329823\n",
      "                  holiday_balance.taken    0.325553\n",
      "             art.course_completion_date    0.323209\n",
      "                   holiday_details.date    0.320248\n",
      "          holiday_balance.using_workday    0.318950\n",
      "                 people.workday_learner    0.312909\n",
      "        people.workday_job_family_group    0.312384\n",
      "        holiday_balance.carried_forward    0.308240\n",
      "           holiday_balance.colleague_id    0.307035\n",
      "                holiday_balance.accrued    0.304142\n",
      "                       art.art_due_date    0.303271\n",
      "             holiday_details.entered_on    0.281736\n",
      "           holiday_details.colleague_id    0.276065\n",
      "            holiday_balance.entitlement    0.263609\n",
      "            people.workday_job_category    0.261247\n",
      "                           art.run_date    0.260525\n",
      "                  people.cost_centre_id    0.259332\n",
      "                         art.start_date    0.254022\n",
      "               holiday_details.taken_tf    0.230279\n",
      "             people.high_lev_workertype    0.228644\n",
      "                        people.job_code    0.215335\n",
      "                        people.on_leave    0.211641\n",
      "           art.course_completion_status    0.207455\n",
      "                        people.division    0.204197\n",
      "                       art.wd_file_date    0.202135\n",
      "                             people.fte    0.197424\n",
      "                         people.layer_5    0.194369\n",
      "                       people.post_code    0.193428\n",
      "                         people.layer_4    0.192035\n",
      "                           people.email    0.191763\n",
      "                           people.grade    0.191489\n",
      "                        people.layer_12    0.188284\n",
      "                     people.location_id    0.187790\n",
      "                     people.position_id    0.187679\n",
      "                            people.city    0.185539\n",
      "              people.line_manager_grade    0.184457\n",
      "                      people.cc_layer_4    0.183275\n",
      "                        people.layer_11    0.180009\n",
      "                      people.job_family    0.176348\n",
      "                     people.worker_type    0.173859\n",
      "                         people.layer_6    0.173739\n",
      "                        people.layer_10    0.172557\n",
      "                     people.oim_learner    0.171546\n",
      "                        people.layer_13    0.171377\n",
      "                   people.location_name    0.169477\n",
      "                         people.layer_7    0.169064\n",
      "                      people.is_onshore    0.168461\n",
      "                 people.layer_5_file_id    0.167014\n",
      "                         people.layer_9    0.163252\n",
      "                 people.layer_4_file_id    0.162451\n",
      "          people.direct_line_manager_id    0.159450\n",
      "                      people.cc_layer_2    0.158000\n",
      "                       people.last_name    0.157085\n",
      "                    people.hub_location    0.155559\n",
      "                      people.cc_layer_3    0.154505\n",
      "                         people.layer_3    0.154378\n",
      "                      people.cc_layer_1    0.152918\n",
      "                         people.layer_8    0.152501\n",
      "                         people.layer_2    0.152229\n",
      "                       people.full_name    0.152110\n",
      "                   art.final_art_status    0.152001\n",
      "           people.original_tenure_start    0.148815\n",
      "      people.compensation_grade_profile    0.147064\n",
      "        people.direct_line_manager_name    0.146217\n",
      "                     people.resource_id    0.145924\n",
      "                     people.data_source    0.145167\n",
      "                    people.learner_type    0.145142\n",
      "                people.layer_10_file_id    0.143541\n",
      "                people.layer_11_file_id    0.140925\n",
      "                              art.group    0.140660\n",
      "                       people.job_title    0.140467\n",
      "                people.layer_12_file_id    0.139160\n",
      "                      people.first_name    0.134402\n",
      "                  people.business_title    0.131278\n",
      "                 people.layer_2_file_id    0.130909\n",
      "                          people.agency    0.130030\n",
      "                 people.layer_7_file_id    0.129445\n",
      "                 people.layer_6_file_id    0.129036\n",
      "                 people.layer_9_file_id    0.126922\n",
      "                people.layer_13_file_id    0.123583\n",
      "                 people.layer_3_file_id    0.122972\n",
      "     people.line_manager_business_title    0.119052\n",
      "                 people.layer_8_file_id    0.118446\n",
      "              people.line_manager_email    0.114305\n",
      "                  people.funding_source    0.114216\n",
      "                        art.course_name    0.114180\n",
      "                 people.colleague_layer    0.094339\n",
      "                      people.is_manager    0.085725\n",
      "                        art.resource_id    0.070903\n",
      "                          art.full_name    0.064140\n",
      "                 art.compliance_program    0.056062\n",
      "\n",
      "Running pipeline with model: all-distilroberta-v1\n",
      "\n",
      "Identifying relevant tables and calculating similarity scores...\n",
      "Similarity Scores and Keys as Table:\n",
      "          Table      Similarity\n",
      "       0.223499          people\n",
      "       0.028033             art\n",
      "       0.270366 holiday_balance\n",
      "holiday_details        0.297723\n",
      "Identifying relevant columns and calculating similarity scores...\n",
      "Similarity Scores for Columns as Table:\n",
      "                                 Column  Similarity\n",
      "                  holiday_details.hours    0.419284\n",
      "     people.contractor_day_rate_banding    0.378429\n",
      "         people.scheduled_working_hours    0.335232\n",
      "holiday_balance.hours_not_using_workday    0.314049\n",
      "                 holiday_balance.booked    0.302503\n",
      "              people.current_start_date    0.299961\n",
      "             holiday_details.entered_on    0.296150\n",
      "        holiday_balance.Unbooked_ex_nuw    0.291776\n",
      "                   holiday_details.date    0.289534\n",
      "   holiday_balance.booked_not_yet_taken    0.287796\n",
      "          holiday_balance.using_workday    0.275101\n",
      "               holiday_balance.unbooked    0.263140\n",
      "                      people.leave_date    0.253575\n",
      "            holiday_details.report_date    0.250796\n",
      "            holiday_balance.report_date    0.236766\n",
      "               holiday_details.taken_tf    0.231371\n",
      "                holiday_balance.accrued    0.230020\n",
      "           holiday_details.colleague_id    0.226591\n",
      "                  holiday_balance.taken    0.224894\n",
      "        holiday_balance.carried_forward    0.220540\n",
      "           holiday_balance.colleague_id    0.213412\n",
      "                  people.cost_centre_id    0.206574\n",
      "                holiday_balance.untaken    0.203687\n",
      "            holiday_balance.entitlement    0.202099\n",
      "        people.workday_job_family_group    0.191814\n",
      "                 people.workday_learner    0.184073\n",
      "            people.workday_job_category    0.176702\n",
      "             art.course_completion_date    0.175649\n",
      "                        people.on_leave    0.175000\n",
      "                         art.start_date    0.154131\n",
      "                       people.post_code    0.145985\n",
      "     people.line_manager_business_title    0.145364\n",
      "                  people.business_title    0.142711\n",
      "              people.line_manager_email    0.142323\n",
      "                  people.funding_source    0.141523\n",
      "                        people.division    0.140396\n",
      "                      people.is_onshore    0.139589\n",
      "                           art.run_date    0.139402\n",
      "                      people.job_family    0.137671\n",
      "                        people.job_code    0.136833\n",
      "                          people.agency    0.134737\n",
      "              people.line_manager_grade    0.132650\n",
      "                             people.fte    0.126410\n",
      "                            people.city    0.120508\n",
      "                           people.email    0.118102\n",
      "                       art.art_due_date    0.114034\n",
      "             people.high_lev_workertype    0.113174\n",
      "                      people.is_manager    0.107669\n",
      "                    people.hub_location    0.104897\n",
      "                       people.job_title    0.101468\n",
      "                       art.wd_file_date    0.098842\n",
      "                           people.grade    0.098777\n",
      "           people.original_tenure_start    0.093830\n",
      "                     people.oim_learner    0.090841\n",
      "                     people.worker_type    0.089061\n",
      "          people.direct_line_manager_id    0.088738\n",
      "           art.course_completion_status    0.085086\n",
      "                       people.last_name    0.084691\n",
      "                         people.layer_5    0.083734\n",
      "                        people.layer_12    0.083007\n",
      "                        people.layer_13    0.082799\n",
      "                      people.first_name    0.081055\n",
      "                        people.layer_11    0.079975\n",
      "                         people.layer_7    0.077237\n",
      "                     people.resource_id    0.075553\n",
      "                     people.position_id    0.075219\n",
      "                     people.data_source    0.074847\n",
      "                         people.layer_2    0.074678\n",
      "                     people.location_id    0.073551\n",
      "                         people.layer_9    0.072177\n",
      "                   art.final_art_status    0.071984\n",
      "                         people.layer_8    0.070001\n",
      "                        people.layer_10    0.068331\n",
      "                         people.layer_4    0.066715\n",
      "                       people.full_name    0.065787\n",
      "                         people.layer_6    0.063966\n",
      "                      people.cc_layer_1    0.063283\n",
      "                         people.layer_3    0.061060\n",
      "        people.direct_line_manager_name    0.060695\n",
      "                      people.cc_layer_4    0.059151\n",
      "                 people.layer_5_file_id    0.056071\n",
      "                people.layer_13_file_id    0.055686\n",
      "                      people.cc_layer_2    0.053830\n",
      "                people.layer_12_file_id    0.052867\n",
      "                people.layer_10_file_id    0.050690\n",
      "                      people.cc_layer_3    0.049622\n",
      "                 people.colleague_layer    0.049610\n",
      "                   people.location_name    0.049209\n",
      "                    people.learner_type    0.048407\n",
      "      people.compensation_grade_profile    0.045872\n",
      "                people.layer_11_file_id    0.044235\n",
      "                 people.layer_4_file_id    0.038689\n",
      "                 people.layer_7_file_id    0.038393\n",
      "                 people.layer_8_file_id    0.038378\n",
      "                 people.layer_9_file_id    0.036268\n",
      "                 people.layer_6_file_id    0.029174\n",
      "                 people.layer_3_file_id    0.020695\n",
      "                 people.layer_2_file_id    0.015863\n",
      "                 art.compliance_program    0.007201\n",
      "                              art.group   -0.002853\n",
      "                        art.course_name   -0.034909\n",
      "                        art.resource_id   -0.041883\n",
      "                          art.full_name   -0.046428\n",
      "\n",
      "Running pipeline with model: t5-base\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e521dc1f7db34eabbf4ae290b6bc73ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2145789fa85b460bba200589ab76e984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2b760d979b410ab8122cd5e79bfcd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb292fa51e04d4eb55f4dc3965ebf9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying relevant tables and calculating similarity scores...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify either decoder_input_ids or decoder_inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 184>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning pipeline with model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    198\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m RAGPipeline(DB_FILE, USER_QUERY, model_name)\n\u001b[0;32m--> 199\u001b[0m table_similarity_scores, column_similarity_scores \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mRAGPipeline.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# print(\"Extracting keywords...\")\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# keywords = self.keyword_extractor.extract_keywords(self.user_query)\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# print(f\"Keywords: {keywords}\")\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdentifying relevant tables and calculating similarity scores...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m     table_similarity_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentify_relevant_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilarity Scores and Keys as Table:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mprint\u001b[39m(table_similarity_df\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mRAGPipeline.identify_relevant_tables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21midentify_relevant_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    132\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 133\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     tables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    135\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mEmbeddingHandler.get_embedding\u001b[0;34m(self, text, description)\u001b[0m\n\u001b[1;32m     48\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(combined_input, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 50\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-mpnet-base-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-distilroberta-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1664\u001b[0m, in \u001b[0;36mT5Model.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1661\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1664\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:990\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    989\u001b[0m     err_msg_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_msg_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124minput_ids or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_msg_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Keyword Extractor\n",
    "class KeywordExtractor:\n",
    "    def __init__(self, method=\"roberta\"):\n",
    "        if method == \"roberta\":\n",
    "            self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "            self.model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        else:\n",
    "            raise ValueError(\"Only 'roberta' method is currently supported.\")\n",
    "\n",
    "    def extract_keywords(self, query):\n",
    "        inputs = self.tokenizer(query, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            token_embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        return query.split()  # Simple split for now (replace with improved logic)\n",
    "\n",
    "# Embedding handler\n",
    "class EmbeddingHandler:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Initialize appropriate models and tokenizers\n",
    "        if model_name == \"roberta-base\":\n",
    "            from transformers import RobertaTokenizer, RobertaModel\n",
    "            self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "            self.model = RobertaModel.from_pretrained(model_name)\n",
    "        elif model_name in [\"all-mpnet-base-v2\", \"all-distilroberta-v1\"]:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "        elif model_name == \"t5-base\":\n",
    "            from transformers import T5Tokenizer, T5Model\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "            self.model = T5Model.from_pretrained(model_name)\n",
    "        elif model_name == \"text-embedding-ada-002\":\n",
    "            import openai\n",
    "            self.openai_api = openai\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} is not supported.\")\n",
    "\n",
    "    def get_embedding(self, text, description=\"\"):\n",
    "        \"\"\"\n",
    "        Get the embedding for the input text and description using the specified model.\n",
    "        \"\"\"\n",
    "        combined_input = f\"{text} - {description}\"\n",
    "\n",
    "        if self.model_name == \"roberta-base\":\n",
    "            # Use RobertaModel\n",
    "            inputs = self.tokenizer(combined_input, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "        elif self.model_name in [\"all-mpnet-base-v2\", \"all-distilroberta-v1\"]:\n",
    "            # Use Sentence-Transformers\n",
    "            return self.model.encode(combined_input)\n",
    "\n",
    "        elif self.model_name == \"t5-base\":\n",
    "            # Use T5Model (Encoder Only)\n",
    "            inputs = self.tokenizer(combined_input, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            with torch.no_grad():\n",
    "                encoder_outputs = self.model.encoder(**inputs)\n",
    "            return encoder_outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "        elif self.model_name == \"text-embedding-ada-002\":\n",
    "            # Use OpenAI Embedding API\n",
    "            response = self.openai_api.Embedding.create(\n",
    "                input=combined_input, model=\"text-embedding-ada-002\"\n",
    "            )\n",
    "            return response['data'][0]['embedding']\n",
    "\n",
    "    def calculate_similarity(self, emb1, emb2):\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between two embeddings.\n",
    "        \"\"\"\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        return cosine_similarity([emb1], [emb2])[0][0]\n",
    "\n",
    "# Schema Extractor\n",
    "class SchemaExtractor:\n",
    "    def __init__(self, db_file):\n",
    "        self.db_file = db_file\n",
    "\n",
    "    def extract_schema(self):\n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        schema = {}\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = cursor.fetchall()\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "            columns = cursor.fetchall()\n",
    "            schema[table_name] = [col[1] for col in columns]\n",
    "        conn.close()\n",
    "        return schema\n",
    "\n",
    "\n",
    "# Connections\n",
    "class Connections:\n",
    "    def __init__(self, db_file):\n",
    "        self.db_file = db_file\n",
    "\n",
    "    def extract_keys(self, table_name):\n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        primary_keys = []\n",
    "        foreign_keys = []\n",
    "\n",
    "        # Extract primary keys\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "        columns = cursor.fetchall()\n",
    "        for col in columns:\n",
    "            if col[-1] == 1:  # Primary key indicator\n",
    "                primary_keys.append(col[1])\n",
    "\n",
    "        # Extract foreign keys\n",
    "        cursor.execute(f\"PRAGMA foreign_key_list({table_name});\")\n",
    "        keys = cursor.fetchall()\n",
    "        for key in keys:\n",
    "            foreign_keys.append({\n",
    "                \"from_column\": key[3],\n",
    "                \"to_table\": key[2],\n",
    "                \"to_column\": key[4]\n",
    "            })\n",
    "\n",
    "        conn.close()\n",
    "        return primary_keys, foreign_keys\n",
    "\n",
    "\n",
    "# RAG Pipeline\n",
    "class RAGPipeline:\n",
    "    def __init__(self, db_file, user_query, model_name, similarity_threshold=0.4):\n",
    "        self.db_file = db_file\n",
    "        self.user_query = user_query\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.embedding_handler = EmbeddingHandler(model_name)\n",
    "        self.schema_extractor = SchemaExtractor(db_file)\n",
    "        self.schema = self.schema_extractor.extract_schema()\n",
    "        self.connections = Connections(db_file)\n",
    "\n",
    "\n",
    "    def identify_relevant_tables(self):\n",
    "        results = []\n",
    "        query_embedding = self.embedding_handler.get_embedding(self.user_query)\n",
    "        tables = []\n",
    "        similarities = []\n",
    "        for table, columns in schema_descriptions.items():\n",
    "            table_embedding = self.embedding_handler.get_embedding(table)\n",
    "            similarity = self.embedding_handler.calculate_similarity(\n",
    "                table_embedding, query_embedding\n",
    "            )\n",
    "            if similarity >= self.similarity_threshold:\n",
    "                tables.append(table)\n",
    "                similarities.append(similarity)\n",
    "            results.append({table, similarity})\n",
    "        return pd.DataFrame(results, columns=[\"Table\", \"Similarity\"])\n",
    "\n",
    "    def identify_relevant_columns(self):\n",
    "        query_embedding = self.embedding_handler.get_embedding(self.user_query)\n",
    "        results = []\n",
    "        for table, cols in self.schema.items():\n",
    "            for column in cols:\n",
    "                column_name = f\"{table}.{column}\"\n",
    "                column_embedding = self.embedding_handler.get_embedding(column_name)\n",
    "                similarity = self.embedding_handler.calculate_similarity(\n",
    "                    column_embedding, query_embedding\n",
    "                )\n",
    "                results.append((column_name, similarity))\n",
    "        \n",
    "        # Sort and select top N columns\n",
    "        sorted_results = sorted(results, key=lambda x: x[1], reverse=True)[:]\n",
    "        return pd.DataFrame(sorted_results, columns=[\"Column\", \"Similarity\"])\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        # print(\"Extracting keywords...\")\n",
    "        # keywords = self.keyword_extractor.extract_keywords(self.user_query)\n",
    "        # print(f\"Keywords: {keywords}\")\n",
    "\n",
    "        print(\"Identifying relevant tables and calculating similarity scores...\")\n",
    "        table_similarity_df = self.identify_relevant_tables()\n",
    "\n",
    "        print(\"Similarity Scores and Keys as Table:\")\n",
    "        print(table_similarity_df.to_string(index=False))\n",
    "\n",
    "        print(\"Identifying relevant columns and calculating similarity scores...\")\n",
    "        column_similarity_df = self.identify_relevant_columns()\n",
    "\n",
    "        print(\"Similarity Scores for Columns as Table:\")\n",
    "        print(column_similarity_df.to_string(index=False))\n",
    "\n",
    "        return table_similarity_df, column_similarity_df\n",
    "\n",
    "# Main Runner\n",
    "if __name__ == \"__main__\":\n",
    "    DB_FILE = \"company_data.db\"\n",
    "    USER_QUERY = \"How many people have booked at least 50 hours before 01/05/2024?\"\n",
    "\n",
    "    models = [\n",
    "        \"roberta-base\",\n",
    "        \"all-mpnet-base-v2\",\n",
    "        \"all-distilroberta-v1\",\n",
    "        \"t5-base\",\n",
    "        \"text-embedding-ada-002\"\n",
    "    ]\n",
    "\n",
    "    for model_name in models:\n",
    "        print(f\"\\nRunning pipeline with model: {model_name}\\n\")\n",
    "        pipeline = RAGPipeline(DB_FILE, USER_QUERY, model_name)\n",
    "        table_similarity_scores, column_similarity_scores = pipeline.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
